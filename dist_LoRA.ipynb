{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJSf0pi4fFUV"
      },
      "outputs": [],
      "source": [
        "#!pip install -q --upgrade keras-nlp\n",
        "#!pip install -q --upgrade keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dTPo9OTEfPLs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import time\n",
        "\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DLKFcbYfZdGa"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hD0qIqFgVt8W"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wLou9S1ffReu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 128\n",
        "MAX_GENERATION_LENGTH = 200\n",
        "\n",
        "RoBERTa_PRESET = \"roberta_base_en\"\n",
        "\n",
        "RANK = 4\n",
        "ALPHA = 32.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vdB9xxmAfoIN"
      },
      "outputs": [],
      "source": [
        "def get_optimizer_and_loss():\n",
        "    optimizer = keras.optimizers.AdamW(\n",
        "        learning_rate=5e-5,\n",
        "        weight_decay=0.01,\n",
        "        epsilon=1e-6,\n",
        "        global_clipnorm=1.0,  # Gradient clipping.\n",
        "    )\n",
        "    # Exclude layernorm and bias terms from weight decay.\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
        "    optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
        "\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    return optimizer, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8jWJGMdlfpjO"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "  preprocessor = keras_nlp.models.RobertaPreprocessor.from_preset(\n",
        "      \"roberta_base_en\",sequence_length=MAX_SEQUENCE_LENGTH\n",
        "  )\n",
        "\n",
        "  classifier = keras_nlp.models.RobertaClassifier.from_preset(\n",
        "      \"roberta_base_en\",\n",
        "      num_classes=3,\n",
        "      preprocessor=preprocessor\n",
        "  )\n",
        "  return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TuBW1_A08v4u"
      },
      "outputs": [],
      "source": [
        "mnli = tfds.load(\"Multi_NLI\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "t5b_ejMo17hV"
      },
      "outputs": [],
      "source": [
        "train_d=mnli[\"train\"]\n",
        "valid_m=mnli[\"validation_matched\"]\n",
        "valid_nm=mnli[\"validation_mismatched\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep_mnli(train,samp):\n",
        "  x=[]\n",
        "  y=[]\n",
        "  check=0\n",
        "  tot=len(train)\n",
        "  for i in train:\n",
        "    x.append((i[\"hypothesis\"].numpy(),i[\"premise\"].numpy()))\n",
        "    y.append(i[\"label\"].numpy())\n",
        "    check+=1\n",
        "    if check==samp:\n",
        "      break\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "CICZf3k9IO2F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sPduTpjiiNAd"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_division(x,y):\n",
        "  x_12, x_3, y_12, y_3 = train_test_split(x, y, test_size=0.3333, random_state=1)\n",
        "\n",
        "  x_1, x_2, y_1, y_2 = train_test_split(x_12, y_12, test_size=0.5, random_state=1)\n",
        "  return ((x_1,y_1),(x_2,y_2),(x_3,y_3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "3p5rN5Y__BUR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class LoraLayer(keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        original_layer,\n",
        "        rank=8,\n",
        "        alpha=32,\n",
        "        trainable=False,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        # We want to keep the name of this layer the same as the original\n",
        "        # dense layer.\n",
        "        original_layer_config = original_layer.get_config()\n",
        "        name = original_layer_config[\"name\"]\n",
        "\n",
        "        kwargs.pop(\"name\", None)\n",
        "\n",
        "        super().__init__(name=name, trainable=trainable, **kwargs)\n",
        "\n",
        "        self.rank = rank\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self._scale = alpha / rank\n",
        "\n",
        "        self._num_heads = original_layer_config[\"output_shape\"][-2]\n",
        "        self._hidden_dim = self._num_heads * original_layer_config[\"output_shape\"][-1]\n",
        "\n",
        "        # Layers.\n",
        "\n",
        "        # Original dense layer.\n",
        "        self.original_layer = original_layer\n",
        "        # No matter whether we are training the model or are in inference mode,\n",
        "        # this layer should be frozen.\n",
        "        self.original_layer.trainable = False\n",
        "\n",
        "        # LoRA dense layers.\n",
        "        self.A = keras.layers.Dense(\n",
        "            units=rank,\n",
        "            use_bias=False,\n",
        "\n",
        "            # Note: the original paper mentions that normal distribution was\n",
        "            # used for initialization. However, the official LoRA implementation\n",
        "            # uses \"Kaiming/He Initialization\".\n",
        "            kernel_initializer=keras.initializers.VarianceScaling(\n",
        "                scale=math.sqrt(5), mode=\"fan_in\", distribution=\"uniform\"\n",
        "            ),\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_A\",\n",
        "        )\n",
        "        # B has the same `equation` and `output_shape` as the original layer.\n",
        "        # `equation = abc,cde->abde`, where `a`: batch size, `b`: sequence\n",
        "        # length, `c`: `hidden_dim`, `d`: `num_heads`,\n",
        "        # `e`: `hidden_dim//num_heads`. The only difference is that in layer `B`,\n",
        "        # `c` represents `rank`.\n",
        "        self.B = keras.layers.EinsumDense(\n",
        "            equation=original_layer_config[\"equation\"],\n",
        "            output_shape=original_layer_config[\"output_shape\"],\n",
        "            kernel_initializer=\"zeros\",\n",
        "            trainable=trainable,\n",
        "            name=f\"lora_B\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        original_output = self.original_layer(inputs)\n",
        "        lora_output = self.B(self.A(inputs)) * self._scale\n",
        "        return original_output + lora_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bRJZSJI45Sd1"
      },
      "outputs": [],
      "source": [
        "def lora(classifier):\n",
        "  for layer_idx in range(classifier.backbone.num_layers):\n",
        "    # Change query dense layer.\n",
        "      decoder_layer = classifier.backbone.get_layer(f\"transformer_layer_{layer_idx}\")\n",
        "      self_attention_layer = decoder_layer._self_attention_layer\n",
        "    # Allow mutation to Keras layer state.\n",
        "      self_attention_layer._tracker.locked = False\n",
        "\n",
        "    # Change query dense layer.\n",
        "      self_attention_layer._query_dense = LoraLayer(\n",
        "        self_attention_layer._query_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )\n",
        "\n",
        "    # Change value dense layer.\n",
        "      self_attention_layer._value_dense = LoraLayer(\n",
        "        self_attention_layer._value_dense,\n",
        "        rank=RANK,\n",
        "        alpha=ALPHA,\n",
        "        trainable=True,\n",
        "    )\n",
        "  for layer in classifier._flatten_layers():\n",
        "    lst_of_sublayers = list(layer._flatten_layers())\n",
        "\n",
        "    if len(lst_of_sublayers) == 1:  # \"leaves of the model\"\n",
        "        if layer.name in [\"lora_A\", \"lora_B\"]:\n",
        "          layer.trainable = True\n",
        "        else:\n",
        "          layer.trainable = False\n",
        "\n",
        "  return classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_zfa6w58oXbe"
      },
      "outputs": [],
      "source": [
        "def batchify(data_div,batch,n):\n",
        "  li=[]\n",
        "  for d in range(n):\n",
        "    x=[data_div[d][0][i] for i in range(len(data_div[d][0]))]\n",
        "    y=[data_div[d][1][i] for i in range(len(data_div[d][1]))]\n",
        "    #print(y)\n",
        "    #print(len(x))\n",
        "    temp=[]\n",
        "    for j in range(0,len(data_div[d][0]),batch):\n",
        "      temp.append([x[j:j+batch],y[j:j+batch]])\n",
        "    #print(len(temp))\n",
        "    li.append(temp)\n",
        "  #print(len(li))\n",
        "  return zip(*li)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def models(n):\n",
        "  liModels=[]\n",
        "  for i in range(n):\n",
        "    keras.backend.clear_session()\n",
        "    liModels.append(get_model())\n",
        "  loraModels=[]\n",
        "  for i in liModels:\n",
        "    keras.backend.clear_session()\n",
        "    loraModels.append(lora(i))\n",
        "  return loraModels"
      ],
      "metadata": {
        "id": "KtojwMz14VJW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ktLHfyN20HOY"
      },
      "outputs": [],
      "source": [
        "def A_matrix(param):\n",
        "  li=[]\n",
        "  for i in range(len(param)):\n",
        "    if param[i][1].path.split(\"/\")[-2] == \"lora_A\":\n",
        "      li.append(param[i])\n",
        "  return li\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HbntG67g_cLd"
      },
      "outputs": [],
      "source": [
        "def B_matrix(param):\n",
        "  li=[]\n",
        "  for i in range(len(param)):\n",
        "    if param[i][1].path.split(\"/\")[-2] == \"lora_B\":\n",
        "      li.append(param[i])\n",
        "  return li"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FH7WeJrHyBW5"
      },
      "outputs": [],
      "source": [
        "def get_grads(model,x,y,loss,loraModels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    x=np.array(x)\n",
        "    pred=model(loraModels[0].preprocessor(x),training=True)\n",
        "    loss_value=loss(np.array(y),pred)\n",
        "  grad=tape.gradient(loss_value,model.trainable_variables)\n",
        "  #print(grad)\n",
        "  return (loss_value,zip(grad,model.trainable_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QqqUpqJDQQ49"
      },
      "outputs": [],
      "source": [
        "def projected_GD(n,lr,np_lis,epochs=20):\n",
        "  li=[0]*n\n",
        "  for i in range(n):\n",
        "    po=np.random.normal((1))\n",
        "    li[i]=tf.Variable(po,tf.float32)\n",
        "  sum=0\n",
        "  for i in range(epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "      for i in range(n):\n",
        "        sum=sum+(li[i]*np_lis[i][0])\n",
        "      sqn=tf.math.square(tf.norm(sum))\n",
        "    grads=tape.gradient(sqn,[i for i in li])\n",
        "    #print(li)\n",
        "    for i in range(n):\n",
        "      li[i].assign_sub(lr*grads[i])\n",
        "    #print(li)\n",
        "    z=[i.numpy() for i in li]\n",
        "    c=projection_simplex_sort(np.array(z))\n",
        "    for i in range(n):\n",
        "      li[i]=tf.Variable(c[i],tf.float32)\n",
        "  fin=0\n",
        "  for i in range(n):\n",
        "    fin=fin+(li[i]*np_lis[i][0])\n",
        "  keras.backend.clear_session()\n",
        "  return fin\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qxXCArcSHmnI"
      },
      "outputs": [],
      "source": [
        "def projection_simplex_sort(v, z=1):\n",
        "    n_features = v.shape[0]\n",
        "    u = np.sort(v)[::-1]\n",
        "    cssv = np.cumsum(u) - z\n",
        "    ind = np.arange(n_features) + 1\n",
        "    cond = u - cssv / ind > 0\n",
        "    rho = ind[cond][-1]\n",
        "    theta = cssv[cond][-1] / float(rho)\n",
        "    w = np.maximum(v - theta, 0)\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(loraModels,D,batch_size,loss,epochs,p_epochs=20,lr=0.001):\n",
        "  opt={i:get_optimizer_and_loss()[0] for i in range(len(loraModels))}\n",
        "  model_train_loss={}\n",
        "\n",
        "  for l_m in range(len(loraModels)):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model_train_loss[l_m]=tf.keras.metrics.Mean(name='mean', dtype=None)\n",
        "  for epy in range(1,epochs+1):\n",
        "    count=0\n",
        "    tf.keras.backend.clear_session()\n",
        "    print(\"Epoch \",epy, \" out of \",epochs)\n",
        "    Dsets=batchify(D,batch_size,len(loraModels))\n",
        "    cut=0\n",
        "    while True:\n",
        "      current_batch=[]\n",
        "      a=next(Dsets,None)\n",
        "      count+=1\n",
        "      if a is None:\n",
        "        cut=1\n",
        "        break\n",
        "      for b in range(len(loraModels)):\n",
        "        current_batch.append(a[b])\n",
        "      g={}\n",
        "      A_lora={}\n",
        "      for d in range(len(current_batch)):\n",
        "        tf.keras.backend.clear_session()\n",
        "        loss_val,g[d]=get_grads(loraModels[d],current_batch[d][0],current_batch[d][1],loss,loraModels)\n",
        "        li_g=list(g[d])\n",
        "        model_train_loss[d].update_state(loss_val)\n",
        "        A_lora[d]=A_matrix(li_g)\n",
        "        #print(\"Hel_2\")\n",
        "        opt[d].apply_gradients(B_matrix(li_g))\n",
        "        print(\"Model \",d,\" batch \",count)\n",
        "      for i in range(len(A_lora[0])):\n",
        "        arr=[]\n",
        "        #up_a=[]\n",
        "        for j in range(len(loraModels)):\n",
        "          arr.append(A_lora[j][i])\n",
        "          #up_a.append((j,A_lora[j][i]))\n",
        "        p=projected_GD(len(loraModels),lr,arr,p_epochs)\n",
        "        #p\n",
        "        for iter in arr:\n",
        "          #print(iter[1][1])\n",
        "          iter[1].assign_sub(0.001*p)\n",
        "        #print(\"Done\")\n",
        "    for avg_loss in range(len(loraModels)):\n",
        "      print(\"Avg_training_loss for model \", avg_loss,\" in epoch \",epy, \" is \", model_train_loss[avg_loss].result().numpy())\n",
        "    for avg_loss in range(len(loraModels)):\n",
        "      model_train_loss[avg_loss].reset_state()\n",
        "  return loraModels\n"
      ],
      "metadata": {
        "id": "QBxgv6lBjTgU"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y=data_prep_mnli(train_d,160)\n",
        "data_div=data_division(x,y)"
      ],
      "metadata": {
        "id": "PYYvayRoI_MX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loraModels=models(3)"
      ],
      "metadata": {
        "id": "iD2Op1D3JSDv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_,loss=get_optimizer_and_loss()"
      ],
      "metadata": {
        "id": "ew2KBygxJrk-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m=train(loraModels,data_div,16,loss,2)"
      ],
      "metadata": {
        "id": "hMOu0dhnJb6U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}